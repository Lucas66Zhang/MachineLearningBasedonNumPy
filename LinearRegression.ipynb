{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim:int=1, \n",
    "                 add_bias:bool=True,\n",
    "                 lmbd:float=0) -> None:\n",
    "        \"\"\"initialize LinearRegression Class\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): dim of feature\n",
    "            output_dim (int): dim of label, the default value is 1\n",
    "            bias (bool, optional): whether to add bias term. Defaults to True.\n",
    "            lmbd (float, optional): the coefficent of regularization tern in loss function. \\\\\n",
    "                Default is 0. When be assigned not to be zero, the model will be ridge regression.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.trained = False\n",
    "        self.add_bias = add_bias\n",
    "        if self.add_bias:\n",
    "            self.weight = np.random.normal(0,1,size=(input_dim+1, output_dim))\n",
    "        else:\n",
    "            self.weight = np.random.normal(0,1,size=(input_dim, output_dim))\n",
    "        self.lmbd = lmbd\n",
    "    \n",
    "    def fit(self, x:np.array, y:np.array) -> None:\n",
    "        \"\"\"fit the model on the training data\n",
    "\n",
    "        Args:\n",
    "            x (np.array): feature matrix (m, n)\n",
    "            y (np.array): label matrix (n, k)\n",
    "        \"\"\"\n",
    "        # reshape y \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # add bias term to feature matrix\n",
    "        if self.add_bias:\n",
    "            x = np.hstack((x, np.ones((x.shape[0], 1))))\n",
    "\n",
    "        # SVD to reduce computation\n",
    "        U, s, Vh = np.linalg.svd(x, full_matrices=False)\n",
    "        V = Vh.T\n",
    "        # get weight\n",
    "        self.weight = Vh.T @ np.diag([i/(i**2 + self.lmbd) for i in s]) @ U.T @ y\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, x:np.array) -> np.array:\n",
    "        \"\"\"predict the lable of new samples\n",
    "\n",
    "        Args:\n",
    "            x (np.array): feature matrix of new samples\n",
    "\n",
    "        Returns:\n",
    "            np.array: predicted labels\n",
    "        \"\"\"\n",
    "        if self.trained:\n",
    "            return x @ self.weight\n",
    "        else:\n",
    "            print(\"The model have not been trained, the output results have no value\")\n",
    "            return x @ self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(5, 5, size=100)\n",
    "y = np.random.normal(x, 0.5)+6\n",
    "x = x.reshape(-1, 1)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00630227],\n",
       "       [5.89130493]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression(input_dim=1,\n",
    "                      output_dim=1,\n",
    "                      lmbd=0.5)\n",
    "lr.fit(x, y)\n",
    "lr.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [ 1., -1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, -1],[2, -2]])\n",
    "a/np.abs(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression():\n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim:int=1, \n",
    "                 add_bias:bool=True,\n",
    "                 lmbd:float=0) -> None:\n",
    "        \"\"\"initialize LinearRegression Class\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): dim of feature\n",
    "            output_dim (int): dim of label, the default value is 1\n",
    "            bias (bool, optional): whether to add bias term. Defaults to True.\n",
    "            lmbd (float, optional): the coefficent of regularization tern in loss function. \\\\\n",
    "                Default is 0. When be assigned not to be zero, the model will be ridge regression.\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.trained = False\n",
    "        self.add_bias = add_bias\n",
    "        if self.add_bias:\n",
    "            self.weight = np.zeros((input_dim+1, output_dim))\n",
    "        else:\n",
    "            self.weight = np.zeros((input_dim, output_dim))\n",
    "        self.lmbd = lmbd\n",
    "    \n",
    "    def fit(self, x:np.array, y:np.array) -> None:\n",
    "        \"\"\"fit the model on the training data\n",
    "\n",
    "        Args:\n",
    "            x (np.array): feature matrix (m, n)\n",
    "            y (np.array): label matrix (n, k)\n",
    "        \"\"\"\n",
    "        # reshape y \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        # add bias term to feature matrix\n",
    "        if self.add_bias:\n",
    "            x = np.hstack((x, np.ones((x.shape[0], 1))))\n",
    "\n",
    "        # SVD to reduce computation\n",
    "        U, s, Vh = np.linalg.svd(x, full_matrices=False)\n",
    "        V = Vh.T\n",
    "        # get weight\n",
    "        self.weight = Vh.T @ np.diag([i/(i**2 + self.lmbd) for i in s]) @ U.T @ y\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, x:np.array) -> np.array:\n",
    "        \"\"\"predict the lable of new samples\n",
    "\n",
    "        Args:\n",
    "            x (np.array): feature matrix of new samples\n",
    "\n",
    "        Returns:\n",
    "            np.array: predicted labels\n",
    "        \"\"\"\n",
    "        if self.trained:\n",
    "            return x @ self.weight\n",
    "        else:\n",
    "            print(\"The model have not been trained, the output results have no value\")\n",
    "            return x @ self.weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression():\n",
    "    def __init__(self,\n",
    "                 input_dim:int,\n",
    "                 output_dim:int,\n",
    "                 add_bias:bool=True,\n",
    "                 lmbd:float=1,\n",
    "                 stop_ctn:float=1e-3,\n",
    "                 lr:float=1e-3,\n",
    "                 max_iter:int=1e4) -> None:\n",
    "        \"\"\"Initialize Lasso Regression class\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): dim of feature\n",
    "            output_dim (int): dim of label, the default value is 1\n",
    "            bias (bool, optional): whether to add bias term. Defaults to True.\n",
    "            lmbd (float, optional): the coefficent of regularization tern in loss function. \\\\\n",
    "                Default is 1. When be assigned not to be zero, the model will be ridge regression.\n",
    "            stop_ctn (float, optional): Default is 1e-3. Be used to determine whether to stop training or not.\n",
    "            lr (float, optinal): Default is 1e-3. Learning rate.\n",
    "            max_iter (int, optional): Default is 1e4. # of max iteration times\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.outpot_dim = output_dim\n",
    "        self.lmbd = lmbd\n",
    "        self.trained = False\n",
    "        self.add_bias = add_bias\n",
    "        if self.add_bias:\n",
    "            self.weight = np.ones(shape=(input_dim+1, output_dim))\n",
    "        else:\n",
    "            self.weight = np.ones(shape=(input_dim, output_dim))\n",
    "        self.stop_ctn = stop_ctn\n",
    "        self.lr = lr\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def loss(self, x:np.array, y:np.array) -> float:\n",
    "        \"\"\"calculate the loss of the model.\n",
    "\n",
    "        Args:\n",
    "            x (np.array): feautre matrix\n",
    "            y (np.array): output label\n",
    "\n",
    "        Returns:\n",
    "            float: loss \n",
    "        \"\"\"\n",
    "        return np.sum((x @ self.weight - y)**2) + self.lmbd * sum(np.abs(self.weight))\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self, x:np.array, y:np.array) -> None:\n",
    "        \"\"\"fit the model on the training data\n",
    "\n",
    "        Args:\n",
    "            x (np.array): input feature matrix\n",
    "            y (np.array): output label\n",
    "        \"\"\"\n",
    "        # reshape y \n",
    "        if len(y.shape) == 1:\n",
    "            y = y.reshape(-1, 1)\n",
    "\n",
    "        sample_num = x.shape[0]\n",
    "        # add bias term to feature matrix\n",
    "        if self.add_bias:\n",
    "            x = np.hstack((x, np.ones((sample_num, 1))))\n",
    "\n",
    "        pre_loss = np.inf\n",
    "        cur_loss = self.loss(x, y)\n",
    "        loss_diff = pre_loss - cur_loss\n",
    "\n",
    "        U, s, Vh = np.linalg.svd(x, full_matrices=False)\n",
    "\n",
    "        iter = 0\n",
    "\n",
    "        while abs(loss_diff) > self.stop_ctn:\n",
    "            weight_abs = np.abs(self.weight)\n",
    "            weight_abs[weight_abs == 0] = 1\n",
    "            grad = 2 * Vh.T @ np.diag([i**2 for i in s]) @ Vh @ self.weight - 2 * x.T @ y + self.lmbd * self.weight / weight_abs\n",
    "            self.weight = self.weight - self.lr * grad / sample_num\n",
    "            pre_loss = cur_loss\n",
    "            cur_loss = self.loss(x, y)\n",
    "            loss_diff = pre_loss - cur_loss\n",
    "            iter =+ 1\n",
    "            if iter >= self.max_iter:\n",
    "                break\n",
    "        self.trained = True\n",
    "        \n",
    "    def predict(self, x:np.array) -> np.array:\n",
    "        \"\"\"predict the label of new samples\n",
    "\n",
    "        Args:\n",
    "            x (np.array): feature matrix of new samples\n",
    "\n",
    "        Returns:\n",
    "            np.array: predicted labels\n",
    "        \"\"\"\n",
    "        if self.add_bias:\n",
    "            x = np.hstack((x, np.ones((x.shape[0], 1))))\n",
    "\n",
    "        if self.trained:\n",
    "            return x @ self.weight\n",
    "        else:\n",
    "            print(\"The model have not been trained, the output results have no value\")\n",
    "            return x @ self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0063245 ],\n",
       "       [5.87758123]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LassoRegression(input_dim=1,\n",
    "                    output_dim=1,\n",
    "                    lmbd=0.5)\n",
    "lr.fit(x, y)\n",
    "lr.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, -1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0,2,-3])\n",
    "a[a<0] = -1\n",
    "a[a>0] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48992502 -0.87176457]\n",
      " [-0.87176457  0.48992502]]\n",
      "[7.34342046 0.27235265]\n",
      "[[-0.60828716 -0.79371704]\n",
      " [ 0.79371704 -0.60828716]]\n",
      "[[8.5]\n",
      " [9.5]]\n",
      "[[8.5]\n",
      " [9.5]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[2],[-1]])\n",
    "x = np.array([[2,3], [4, 5]])\n",
    "\n",
    "u, s, v = np.linalg.svd(x)\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)\n",
    "\n",
    "y = np.array([[1],[2]])\n",
    "print(2*x.T@x@w - 2*x.T@y + 0.5*w/np.abs(w))\n",
    "print(2*v.T@np.diag([i**2 for i in s])@v@w  - 2*x.T@y + 0.5*w/np.abs(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14]\n",
      " [18]]\n",
      "[[10]\n",
      " [13]]\n"
     ]
    }
   ],
   "source": [
    "print(x.T@x@w)\n",
    "print(x.T@y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a6b081c12a093feba9d2034b31ec6857e724f59c488a8014a2dff341d174ebf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
